---
layout: post
title: AA-sort
date: '2009-02-13T22:35:00.011+09:00'
author: 'Contact:'
tags:
- cbe
modified_time: '2009-02-14T00:15:08.643+09:00'
blogger_id: tag:blogger.com,1999:blog-5371326417913853395.post-4670413677275359933
blogger_orig_url: http://rendered-obsolete.blogspot.com/2009/02/aa-sort.html
---

A co-worker sent me a link to <a href="http://www.trl.ibm.com/people/inouehrs/pdf/PACT2007-SIMDsort.pdf">AA-sort</a> some time ago, but I didn't have any reason to use it until just recently.  I found myself in a situation where I have 2K~4K 4-byte elements packet into qwords and my initial O(N^2) solution just wasn't working out.  So, I decided to try the O(N log N) "in-core" component of AA-sort as a pre-sort.<div><br /></div><div>The paper linked above is relatively straight-forward, but they only provide pseudo-code so there's still a bit of effort required to get something usable.  As per the author's goals, it's easily written as highly-vectorized, non-branching code and was rather fun to get working.</div><div><br /></div><div>The authors were working with 8 16-bit values packed into a qword, and I had to make some simple changes to get it working with vec_uint4s.</div><div><br /></div><div>First, my bitonic_sort() routine takes care of step one of their in-core algorithm by sorting the 4 uint32_t elements in a qword:</div><br /><pre class="code">inline vec_uint4 bitonic_sort( vec_uint4 Vec_ )<br />{<br />   static const vec_uchar16 BADC = {<br />       0x04, 0x05, 0x06, 0x07, 0x00, 0x01, 0x02, 0x03,<br />       0x0c, 0x0d, 0x0e, 0x0f, 0x08, 0x09, 0x0a, 0x0b<br />   };<br />   static const vec_uchar16 CDAB = {<br />       0x08, 0x09, 0x0a, 0x0b, 0x0c, 0x0d, 0x0e, 0x0f,<br />       0x00, 0x01, 0x02, 0x03, 0x04, 0x05, 0x06, 0x07<br />   };<br />   static const vec_uint4 mask1 = {<br />       0x00000000, 0xffffffff, 0xffffffff, 0x00000000<br />   };<br />   static const vec_uint4 mask2 = {<br />       0x00000000, 0x00000000, 0xffffffff, 0xffffffff<br />   };<br />   static const vec_uint4 mask3 = {<br />       0x00000000, 0xffffffff, 0x00000000, 0xffffffff<br />   };<br />   vec_uint4 temp, cmp, result;<br /><br />   temp = spu_shuffle( Vec_, Vec_, BADC ); // Compare A to B and C to D<br />   cmp = spu_cmpgt( Vec_, temp );<br />   cmp = spu_xor( cmp, mask1 ); // Order A/B increasing, C/D decreasing<br />   result = spu_sel( Vec_, temp, cmp );<br /><br />   temp = spu_shuffle( result, result, CDAB ); // Compare AB to CD<br />   cmp2 = spu_cmpgt( result, temp );<br />   cmp2 = spu_xor( cmp2, mask2 ); // Order AB/CD increasing<br />   result = spu_sel( result, temp, cmp2 );<br />   temp = spu_shuffle( result, result, BADC ); // Compare previous A to B and C to D<br />   cmp = spu_cmpgt( result, temp );<br />   cmp = spu_xor( cmp, mask3 ); // Order A/B and C/D increasing<br />   result = spu_sel( result, temp, cmp );<br /><br />   return result;<br />}<br /></pre><br />Next come cmpswap() and cmpswap_skew() from the paper:<br /><br /><pre class="code">inline vec_uint4 vector_cmpswap( vec_uint4* A_, vec_uint4* B_ )<br />{<br />   const vec_uint4 a = *A_;<br />   const vec_uint4 b = *B_;<br />   const vec_uint4 cmp = spu_cmpgt( a, b );<br />   *A_ = spu_sel( a, b, cmp );<br />   *B_ = spu_sel( b, a, cmp );<br /><br />   return cmp;<br />}<br /><br />inline vec_uint4 vector_cmpswap_skew( vec_uint4* A_, vec_uint4* B_ )<br />{<br />   const vec_uint4 a = *A_;<br />   const vec_uint4 b = *B_;<br /><br />   // The last word compared is 0xffff so that part of the mask should always be 0000<br />   static const vec_uchar16 BCDx = {<br />       0x04, 0x05, 0x06, 0x07, 0x08, 0x09, 0x0a, 0x0b,<br />       0x0c, 0x0d, 0x0e, 0x0f, 0xc0, 0xc0, 0xc0, 0xc0<br />   };<br />   const vec_uint4 bShift = spu_shuffle( b, b, BCDx );<br />   const vec_uint4 cmp = spu_cmpgt( a, bShift );<br />   const vec_uint4 swap = spu_sel( a, bShift, cmp );<br />   const vec_uint4 bSwap = spu_sel( bShift, a, cmp );<br /><br />   static const vec_uchar16 abcD = {<br />       0x10, 0x11, 0x12, 0x13, 0x14, 0x15, 0x16, 0x17,<br />       0x18, 0x19, 0x1a, 0x1b, 0x0c, 0x0d, 0x0e, 0x0f<br />   };<br />   static const vec_uchar16 Aabc = {<br />       0x00, 0x01, 0x02, 0x03, 0x10, 0x11, 0x12, 0x13,<br />       0x14, 0x15, 0x16, 0x17, 0x18, 0x19, 0x1a, 0x1b<br />   };<br />   *A_ = spu_shuffle( a, swap, abcD );<br />   *B_ = spu_shuffle( b, bSwap, Aabc );<br /><br />   return cmp;<br />}<br /></pre><br />In both functions I return the comparison mask because I think it's necessary to implement step 2 of AA-sort's in-core algorithm; the modified combsort:<br /><br /><pre class="code">static const float INV_SHRINK_FACTOR = 1.f/1.3f;<br />uint32_t gap = uint32_t(float(edgeCount) * INV_SHRINK_FACTOR);<br />while (gap &gt; 1)<br />{<br />   // Straight comparisons<br />   const uint32_t maxSwapIndex = edgeCount - gap;<br />   for (uint32_t i = 0; i &lt; maxSwapIndex; ++i)<br />   {<br />       vector_cmpswap( (vec_uint4*)&amp;work[i], (vec_uint4*)&amp;work[i+gap] );<br />   }<br /><br />   // Skewed comparisons for when i+gap exceeds N/4<br />   for (uint32_t i = edgeCount - gap; i &lt; edgeCount; ++i)<br />   {<br />       vector_cmpswap_skew( (vec_uint4*)&amp;work[i], (vec_uint4*)&amp;work[i+gap - edgeCount] );<br />   }<br /><br />   gap = uint32_t((float)gap * INV_SHRINK_FACTOR);<br />}<br />vec_uint4 not_sorted;<br />do<br />{<br />   not_sorted = spu_splats((uint32_t)0);<br />   for (uint32_t i = 0; i &lt; edgeCount - 1; ++i)<br />   {<br />       not_sorted = spu_or( not_sorted, vector_cmpswap( (vec_uint4*)&amp;work[i], (vec_uint4*)&amp;work[i+1] ) );<br />   }<br />   not_sorted = spu_or( not_sorted, vector_cmpswap_skew( (vec_uint4*)&amp;work[edgeCount - 1], (vec_uint4*)&amp;work[0] ) );<br />   not_sorted = spu_orx( not_sorted );<br />} while ( spu_extract(not_sorted, 0) );<br /></pre><br />I use a shrink factor of 1.3 as suggested by the authors as well the <a href="http://en.wikipedia.org/wiki/Combsort">combsort page at Wikipedia</a>.  Standard combsort stops looping once <i>gap</i> is less-or-equal to 1 and no swaps have been performed in the current iteration.  AA-sort splits the <i>gap</i> and "swap" loops so I use the <i>not_sorted</i> value to keep track of when swaps stop occurring (this is the reason that I return the <i>cmp</i> value from each of the swap() functions).<br /><br />You end up with nicely sorted (albeit transposed) data like:<br /><br /><pre class="code">[0] = 00000003 03910392 04a504a6 05140515<br />[1] = 0000002b 03910393 04a604a7 05140516<br />[2] = 0000002b 03910393 04a7049f 05150514<br />[3] = 0000002c 03910394 04a704a8 05150516<br />[4] = 0000002c 03920393 04a704a8 05150516<br />[5] = 0000002d 03930394 04a804a9 05160515<br />[6] = 0003002b 03b903ba 04a9049f 05170516<br />[7] = 00200021 03b903bb 04a904aa 05170518<br />[8] = 00200022 03b903bb 04a904aa 05180517<br />[9] = 00210022 03b903bc 04aa04ab 05190518<br />...<br /></pre><br />AA-sort normally calls for a final pass that transposes the elements across the width of each vector rather than through each vector element but it wasn't necessary in my particular situation.<div><br /></div><div>I took some performance statistics with the decrementer using a few of my data sets and attached the results below:</div><br /><br /><table border="1"><tr><th>Time (ms)</th><th># Values</th></tr><tr><td>0.036667</td><td>1254</td></tr><tr><td>0.008571</td><td>171</td></tr><tr><td>0.017419</td><td>684</td></tr><tr><td>0.131654</td><td>1902</td></tr><tr><td>0.019900</td><td>786</td></tr><tr><td>0.009424</td><td>360</td></tr><tr><td>0.109599</td><td>954</td></tr><tr><td>0.347531</td><td>2415</td></tr><tr><td>0.371779</td><td>2454</td></tr></table><br /><br />Pretty decent considering the original O(N^2) implementation was taking 3 ms for 2k+ values.