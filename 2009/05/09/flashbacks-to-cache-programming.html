<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!-- Begin Jekyll SEO tag v2.4.0 -->
<title>Flashbacks to Cache Programming | Rendered Obsolete</title>
<meta name="generator" content="Jekyll v3.7.2" />
<meta property="og:title" content="Flashbacks to Cache Programming" />
<meta name="author" content="Contact:" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="I recently went back through two classic resources programming considerations for processor caches: “The Elements of Cache Programming Style” by Chris Spears, and Christer Ericson’s “Memory Optimization” talk from GDC 2003. This was partially for my own amusement but also to see if there were any fun tid-bits of information that I had forgotten about. Both can be found rather easily via Google.Both have dated examples using processors from yesteryear, but the core concepts are still relevant today as little has changed in the overall design of the memory hierarchy. In fact, they are perhaps even more relevant today since the widening gap between processor performance and memory latency they they spoke of as looming on the horizon is now upon us. While this has been partially addressed by adding an L3 cache, the many-threaded architecture of modern GPUs, to a lesser extent by SMT, and essentially side-stepped by architectures like the Cell’s SPEs, the dreaded cache-miss bogeyman is still lurking under every bed and in every closet.Both discussed watching structure member alignment and packing to reign in bloat and its impact on cache performance, something that I think people are largely already doing to reduce memory usage. Reorganizing them based on member access ordering and splitting into “hot” and “cold” areas, perhaps less so. The one thing in particular I had forgotten about was Ericson’s suggestion of accessing all member variables through accessors and then instrumenting them looking for frequency and correlation statistics to determine “hotness” and order relationships, respectively. This could perhaps be done mostly automatically with gcc’s -finstrument-functions or the like.Another interesting (obvious?) suggestion was using a custom slab allocator for dynamic data structures so the various link elements all end up in the same page. Traversing linked-lists and other data structures with indirection have obvious cache miss penalties, but the suggestion is specifically meant to potentially reduce the number of cache misses a little and definitely reduce the number of TLB misses. Further dynamic memory should be freed as soon as possible and re-allocated first-fit in order to try and reuse cachelines (there are other, more global considerations such as fragmentation here but for the sake of argument we’re only talking about cache implications). I’m interested to try and instrument our current project to see if this is much of an issue for us.Finally, using gcc’s attribute((section(“name”))), linking order, and other approaches to try and reduce instruction cache misses by arranging functions in a cache friendly manner. Although I’ve read this is largely automated by Link Time Code Generation (Whole Program Optimization), it’s probably relevant for gcc or whenever you need fine-grained control.As an aside, Agner Fog (who has some fantastic, interesting resources at http://www.agner.org/optimize/) advocates “reusing the stack”. Namely, the stack is “free”d and then it and it’s allocated cache lines are reused by consecutive functions. I need a bit more time to think about how this can be used more concretely.I plan to wrap up this journey glancing over a PowerPC 750 related post from IBM. Again, rather old, but I’m hoping it gives me some ideas for my notes own notes on the 970." />
<meta property="og:description" content="I recently went back through two classic resources programming considerations for processor caches: “The Elements of Cache Programming Style” by Chris Spears, and Christer Ericson’s “Memory Optimization” talk from GDC 2003. This was partially for my own amusement but also to see if there were any fun tid-bits of information that I had forgotten about. Both can be found rather easily via Google.Both have dated examples using processors from yesteryear, but the core concepts are still relevant today as little has changed in the overall design of the memory hierarchy. In fact, they are perhaps even more relevant today since the widening gap between processor performance and memory latency they they spoke of as looming on the horizon is now upon us. While this has been partially addressed by adding an L3 cache, the many-threaded architecture of modern GPUs, to a lesser extent by SMT, and essentially side-stepped by architectures like the Cell’s SPEs, the dreaded cache-miss bogeyman is still lurking under every bed and in every closet.Both discussed watching structure member alignment and packing to reign in bloat and its impact on cache performance, something that I think people are largely already doing to reduce memory usage. Reorganizing them based on member access ordering and splitting into “hot” and “cold” areas, perhaps less so. The one thing in particular I had forgotten about was Ericson’s suggestion of accessing all member variables through accessors and then instrumenting them looking for frequency and correlation statistics to determine “hotness” and order relationships, respectively. This could perhaps be done mostly automatically with gcc’s -finstrument-functions or the like.Another interesting (obvious?) suggestion was using a custom slab allocator for dynamic data structures so the various link elements all end up in the same page. Traversing linked-lists and other data structures with indirection have obvious cache miss penalties, but the suggestion is specifically meant to potentially reduce the number of cache misses a little and definitely reduce the number of TLB misses. Further dynamic memory should be freed as soon as possible and re-allocated first-fit in order to try and reuse cachelines (there are other, more global considerations such as fragmentation here but for the sake of argument we’re only talking about cache implications). I’m interested to try and instrument our current project to see if this is much of an issue for us.Finally, using gcc’s attribute((section(“name”))), linking order, and other approaches to try and reduce instruction cache misses by arranging functions in a cache friendly manner. Although I’ve read this is largely automated by Link Time Code Generation (Whole Program Optimization), it’s probably relevant for gcc or whenever you need fine-grained control.As an aside, Agner Fog (who has some fantastic, interesting resources at http://www.agner.org/optimize/) advocates “reusing the stack”. Namely, the stack is “free”d and then it and it’s allocated cache lines are reused by consecutive functions. I need a bit more time to think about how this can be used more concretely.I plan to wrap up this journey glancing over a PowerPC 750 related post from IBM. Again, rather old, but I’m hoping it gives me some ideas for my notes own notes on the 970." />
<link rel="canonical" href="http://localhost:4000/2009/05/09/flashbacks-to-cache-programming.html" />
<meta property="og:url" content="http://localhost:4000/2009/05/09/flashbacks-to-cache-programming.html" />
<meta property="og:site_name" content="Rendered Obsolete" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2009-05-09T21:23:00+08:00" />
<script type="application/ld+json">
{"description":"I recently went back through two classic resources programming considerations for processor caches: “The Elements of Cache Programming Style” by Chris Spears, and Christer Ericson’s “Memory Optimization” talk from GDC 2003. This was partially for my own amusement but also to see if there were any fun tid-bits of information that I had forgotten about. Both can be found rather easily via Google.Both have dated examples using processors from yesteryear, but the core concepts are still relevant today as little has changed in the overall design of the memory hierarchy. In fact, they are perhaps even more relevant today since the widening gap between processor performance and memory latency they they spoke of as looming on the horizon is now upon us. While this has been partially addressed by adding an L3 cache, the many-threaded architecture of modern GPUs, to a lesser extent by SMT, and essentially side-stepped by architectures like the Cell’s SPEs, the dreaded cache-miss bogeyman is still lurking under every bed and in every closet.Both discussed watching structure member alignment and packing to reign in bloat and its impact on cache performance, something that I think people are largely already doing to reduce memory usage. Reorganizing them based on member access ordering and splitting into “hot” and “cold” areas, perhaps less so. The one thing in particular I had forgotten about was Ericson’s suggestion of accessing all member variables through accessors and then instrumenting them looking for frequency and correlation statistics to determine “hotness” and order relationships, respectively. This could perhaps be done mostly automatically with gcc’s -finstrument-functions or the like.Another interesting (obvious?) suggestion was using a custom slab allocator for dynamic data structures so the various link elements all end up in the same page. Traversing linked-lists and other data structures with indirection have obvious cache miss penalties, but the suggestion is specifically meant to potentially reduce the number of cache misses a little and definitely reduce the number of TLB misses. Further dynamic memory should be freed as soon as possible and re-allocated first-fit in order to try and reuse cachelines (there are other, more global considerations such as fragmentation here but for the sake of argument we’re only talking about cache implications). I’m interested to try and instrument our current project to see if this is much of an issue for us.Finally, using gcc’s attribute((section(“name”))), linking order, and other approaches to try and reduce instruction cache misses by arranging functions in a cache friendly manner. Although I’ve read this is largely automated by Link Time Code Generation (Whole Program Optimization), it’s probably relevant for gcc or whenever you need fine-grained control.As an aside, Agner Fog (who has some fantastic, interesting resources at http://www.agner.org/optimize/) advocates “reusing the stack”. Namely, the stack is “free”d and then it and it’s allocated cache lines are reused by consecutive functions. I need a bit more time to think about how this can be used more concretely.I plan to wrap up this journey glancing over a PowerPC 750 related post from IBM. Again, rather old, but I’m hoping it gives me some ideas for my notes own notes on the 970.","author":{"@type":"Person","name":"Contact:"},"@type":"BlogPosting","url":"http://localhost:4000/2009/05/09/flashbacks-to-cache-programming.html","headline":"Flashbacks to Cache Programming","dateModified":"2009-05-09T21:23:00+08:00","datePublished":"2009-05-09T21:23:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2009/05/09/flashbacks-to-cache-programming.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->

  <link rel="stylesheet" href="/assets/main.css">
  <link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Rendered Obsolete" />
  
</head>


  <body>

    <header class="site-header" role="banner">

  <div class="wrapper">
    
    
    <a class="site-title" rel="author" href="/">Rendered Obsolete</a>

    
      <nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
              <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
              <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger">
          
            
            
          
            
            
            <a class="page-link" href="/about/">About</a>
            
          
            
            
          
            
            
          
            
            
          
        </div>
      </nav>
    
  </div>
</header>


    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Flashbacks to Cache Programming</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2009-05-09T21:23:00+08:00" itemprop="datePublished">
        
        May 9, 2009
      </time>
      
        • <span itemprop="author" itemscope itemtype="http://schema.org/Person"><span class="p-author h-card" itemprop="name">Contact:</span></span>
      </p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    I recently went back through two classic resources programming considerations for processor caches: "The Elements of Cache Programming Style" by Chris Spears, and Christer Ericson's "Memory Optimization" talk from GDC 2003.  This was partially for my own amusement but also to see if there were any fun tid-bits of information that I had forgotten about.  Both can be found rather easily via Google.<br /><br />Both have dated examples using processors from yesteryear, but the core concepts are still relevant today as little has changed in the overall design of the memory hierarchy.  In fact, they are perhaps even more relevant today since the widening gap between processor performance and memory latency they they spoke of as looming on the horizon is now upon us.  While this has been partially addressed by adding an L3 cache, the many-threaded architecture of modern GPUs, to a lesser extent by SMT, and essentially side-stepped by architectures like the Cell's SPEs, the dreaded cache-miss bogeyman is still lurking under every bed and in every closet.<br /><br />Both discussed watching structure member alignment and packing to reign in bloat and its impact on cache performance, something that I think people are largely already doing to reduce memory usage.  Reorganizing them based on member access ordering and splitting into "hot" and "cold" areas, perhaps less so.  The one thing in particular I had forgotten about was Ericson's suggestion of accessing all member variables through accessors and then instrumenting them looking for frequency and correlation statistics to determine "hotness" and order relationships, respectively.  This could perhaps be done mostly automatically with gcc's -finstrument-functions or the like.<br /><br />Another interesting (obvious?) suggestion was using a custom slab allocator for dynamic data structures so the various link elements all end up in the same page.  Traversing linked-lists and other data structures with indirection have obvious cache miss penalties, but the suggestion is specifically meant to potentially reduce the number of cache misses a little and definitely reduce the number of TLB misses.  Further dynamic memory should be freed as soon as possible and re-allocated first-fit in order to try and reuse cachelines (there are other, more global considerations such as fragmentation here but for the sake of argument we're only talking about cache implications).  I'm interested to try and instrument our current project to see if this is much of an issue for us.<br /><br />Finally, using gcc's __attribute__((section("name"))), linking order, and other approaches to try and reduce instruction cache misses by arranging functions in a cache friendly manner.  Although I've read this is largely automated by <a href="http://msdn.microsoft.com/en-us/library/xbf3tbeh(VS.80).aspx">Link Time Code Generation</a> (Whole Program Optimization), it's probably relevant for gcc or whenever you need fine-grained control.<br /><br />As an aside, Agner Fog (who has some fantastic, interesting resources at <a href="http://www.agner.org/optimize/">http://www.agner.org/optimize/</a>) advocates "reusing the stack".  Namely, the stack is "free"d and then it and it's allocated cache lines are reused by consecutive functions.  I need a bit more time to think about how this can be used more concretely.<br /><br />I plan to wrap up this journey glancing over a <a href="http://www.ibm.com/developerworks/power/library/pa-ppccache.html">PowerPC 750 related post from IBM</a>.  Again, rather old, but I'm hoping it gives me some ideas for my notes own notes on the 970.
  </div>

  

  <a class="u-url" href="/2009/05/09/flashbacks-to-cache-programming.html" hidden></a>
</article>

      </div>
    </main>

    <footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Rendered Obsolete</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">
            
              Rendered Obsolete
            
            </li>
            
            <li><a class="u-email" href="mailto:your-email@example.com">your-email@example.com</a></li>
            
        </ul>
      </div>

      <div class="footer-col footer-col-2">
        <ul class="social-media-list">
  
  
  
  
  
  
  
  
  
  
  
</ul>

      </div>

      <div class="footer-col footer-col-3">
        <p>Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description.</p>
      </div>
    </div>

  </div>

</footer>


  </body>

</html>
